\chapter{Introduction}

\section{A Brief History of this Thesis}

Back in 2011 I was fresh out of college and, having majored in two different courses, Applied Maths and Computer Engineering, I felt ready to face a bigger challenge. I applied for my Ph.D. at the University of São Paulo after talking to Professor Roberto. By the time, we discussed a few possibilities for a thesis subject and I made clear that I wanted something industry related. Professor Roberto then suggested that I could work with Dr. Claudio Pinhanez, a former professor at the Maths Department at the University of São Paulo that by then was a researcher at IBM Research Brazil. I was more than pleased with this plan.

After about one year into the program, I started interning at IBM Research Brazil. It was then that this thesis started to take form. By the time, Facebook was already the major online social network and the idea that modeling users' online behavior would be essential in the near future for the majority of business was something I had clear in my mind. Certainly, I was not the only one to think so, and there were many researchers already studying social networks such as Twitter, Wikipedia, LastFM, among others. Recommender systems were popular, as well as studies trying to predict virality of tweets. 

My initial assignments as an intern were to craw data from the Twitter network that we initially used for a study on the Obama's and Romney's campaigns. Later on, this same data was used for the reaction detection work in this thesis. I also studied hidden Markov models in order to model users' behavior, thinking of the messages they authored as the observables for the model and their hidden states as their state of mind. This work did not grow further, but lead me to think of predicting weather or not a tweet message would be retweeted.

This idea of predicting retweets however had been recently explored by the time. One thing though that was common in all the works by then was that none of them had a user-centered approach. they all gathered a lot of data and tried to find one classifier that would predict retweets or replies for any given message. Since I was also studying hidden Markov models, graphical networks were fresh on my mind, and this lead to the proposal I made in my qualifying exam: create a graphical model for each user that would be used to classify his or her tweets and predict weather or not it would be retweeted/replied. 

Soon after my qualifying exam, my internship at IBM ended and I started working at a startup named GetNinjas. At the same time I was in contact with Professor Dan Cosley from Cornell University, to whom Dr. David Millen introduced me during my time at IBM. Professor Dan agreed to have me as a foreign student for one year, and six months after IBM I was on my way to Cornell. During this time, I focused on developing a geo-constrained distribution system for GetNinjas, that is a online services marketplace.

Arriving to Cornell, I started working at the Department of Information Science, together with Professor Dan Cosley. In retrospect, now I find that my thesis would be better contextualized in the Information Sciences department rather than the Computer Science, but I only realized this later on. 

In the first months there, I worked on the proposal I had made during my qualifying exam. Unfortunately, the results of my proposed method were less than exciting: I often did not have enough data to properly train a bayesian network and when I had, sometimes my variable of interest, the retweet action, would often be eliminated by a significance threshold. Discussing these issues with Dan, I dropped this approach. Also, a new idea came around, one that we had never seen anyone addressing before: sometimes users are reacting to things they have seen or been exposed to, but these reactions are not necessarily captured by the social networks explicit tools for reactions. We called these reactions implicit reactions.

Based on this idea, we proposed a method based on text similarity that would use ego-networks, that is, a user-centered approach, that would detect these implicit reactions that we did not capture when looking only for retweets and replies. Since we were taking a user-centered approach, we were also able to measure how some users could be invisible if we only looked at explicit reactions, and we saw that some users consistently avoided explicit reaction mechanisms, putting a lot more effort in copying and pasting rather than just clicking a "reply" or "retweet" button. These ideas were published in the 2015 e-Science conference and are part of this thesis.

By this time, I learnt of a large reddit dataset though another student at Cornell. Dan suggested that I explored the idea of novelty of content to predict the popularity of a new message. To do this, I time ordered reddit's messages and created time windows, trying to detect weather or not messages that had no similars in the network by the time it was posted would be somehow more likely to propagate than messages that were more similar to the existing content by the time. Unfortunately we could not establish any correlation with novelty and the likelihood of propagating.

At the same time, the idea of users putting more or less effort into the network from the Twitter work made me think that users could be somehow optimizing their effort to reach some objective. I started trying to model users as a entities that have a bias towards some topics, and applied Latent Dirichlet Allocation to find "topics" over users. The idea was that a user could be seen as a document, and each time a user posted in a subreddit (similar to a forum in reddit, that is a collection of multiple subreddits) I could consider that a "word" in the document, and this word would be the subreddit itself. Based on this, I could find topics of subreddits, clustering subreddits based on users' behavior. I wanted to show that a user would optimize their effort if they posted in the subreddits in these clusters.

While working with these ideas, however, I faced many pitfalls when aggregating reddit's data (more than a billion comments), and realized that the behavior of users changed significantly over time. These pitfalls often hid important trends of growth or decrease of activity of users over time in the measurements of effort I was trying to capture. We realized that these pitfalls were in fact occurrences of the Simpson's Paradox. We organized these pitfalls and the interesting trends we discovered about how users evolved in reddit and published it in the 2016 WWW Conference. My favorite take away of this work is that the newer generations that join the social network are likely to become the majority of the network, and when looking at the average behavior of users, they will dominate the overall trend. This means that whenever you design your product or strategies, you should always think more of who is going to join the network rather than who will stay in it, at least for the time it is growing exponentially.

By the time of WWW conference, I was already back in Brazil. As soon as I arrived, GetNinjas got in touch with me and hired me as a Data Scientist. I also started working with their marketing system, mainly AdWords. This gave me a very clear idea of how to put all this work in practice: modeling users is a extremely powerful tool to optimize marketing strategies. I worked developing a bid optimization system and a campaign creation system for extremely geo-constrained business. I wrote a proposal for the research funding agency FAPESP to help us fund the development of this system through the PIPE program\footnote{This is a program to help develop research and new technology in small and medium business. More information can be found in \url{http://www.fapesp.br/pipe/}.} and by the time of the defense of this thesis, we have been approved for the program.

This summarizes the trajectory of how this thesis came to be and how the works presented here are connected in a larger context. I am greatly thankful for all the people that helped me and made all this possible.

\section{Better Understanding Users' Behavior}

Users' behavior span over a multitude of actions on social networks \cite{Benevenuto2009, Gilbert2009, Comarela2012}: they search for friends, write messages, post images, videos and sounds among many other possibilities. Much of this content is captured by social networks mechanisms that explicitly tag and identify characteristics of the users' interaction. These mechanisms, however, are limited in capturing users' intention and diverse behavior. For instance, researchers have found that users might use references to content that only a specific audience from their peers can understand, turning a common context into a tool of privacy in public contexts \cite{Boyd2011}. The problem of identifying part of these interactions that are not fully captured is important to better understand users as well as to provide new perspectives on what kind of features social networks should provide. This is the main issue addressed in this thesis.

Research has shown that not accounting for time can lead to mistakes when dealing with social networks. Just as with offline contexts, systems and society changes over the years, as well younger generations have a different behavior from older generations. More than just considering different snapshots over time, we analyze users that joined at different stages of the network evolution. Just as with missed reactions, considering time-evolving users is important to reveal behavior that otherwise would not be noticed. Using simple cohorting methods, we demonstrate how different this behavior can be and how easy it is to draw wrong conclusions from averaging practices.

In the first part of this thesis, we address the problem of missed reactions, proposing a text similarity method to identify missed reactions and validating it on Twitter data. We show that a considerable number of users' reactions are not captured by current mechanisms in Twitter and that some types of users are significantly underrepresented based solely on these metrics. In the second part of this thesis, we analyze the users' evolution from a cohort perspective built on top of the time that they joined the network. We show, in the context of Reddit, how users' behavior vary depending on the year that they joined the network as well as how misleading not accounting for time can be. 

This thesis was made in collaboration with IBM Research Brazil and Cornell University. The author interned at IBM Research for 2 years under the advisement of Claudio Pinhanez and visited Cornell University for 1 year as part of the sandwich Ph.D. program, under the advisement of Dan Cosley.

\section{Objectives}

The objective of this thesis is to improve our understanding of users' behavior on social networks. More specifically, to understand the missed behavior in the form of reactions that are not captured and temporal trends that are missed due to poor aggregation of data. We developed methods that help us to capture and identify missed reactions and propose analysis tools that allow us to avoid misunderstanding our data.

\section{Contributions}

We can summarize the contributions of this thesis as the following:

\begin{itemize}
	\item Proposal of a new problem: understanding users' indirect reactions.
	\item Development of a method to detect indirect reactions in the context of Twitter, revealing about 11\% of missed reactions, as well as patterns of behavior that are common, but not modeled, e.g., group conversations.
	\item A cohorted view of the users' evolution on Reddit over 7 years, revealing different behavioral patterns as a function of the users' tenure in the network.
	\item Practical examples of common aggregation practices that lead to wrong conclusions when dealing with time series.
\end{itemize}

The ideas proposed in this thesis have been published in \cite{BarbosaNeto2013, Barbosa, Barbosa2016}.

\section{Organization of this Thesis}

We dedicate Chapter \ref{ch:reactions} of this thesis to the problem of detecting missed reaction. We propose a method based on text similarity to detect reactions other than retweets and replies in Twitter. We show that a significant amount of reactions are being missed. Furthermore, we show that many users consistently react in ways that are not captured by these mechanisms.

In Chapter \ref{ch:cohorts}, we cohort users in Reddit based on their creation and survival years. This analysis shows that naive aggregation of data can lead to wrong conclusions. It is shown that the proposed method can reveal users' evolution trends that would be otherwise missed. The proposed approach highlights the significant role of users joining and leaving the network in shaping the overall behavior.

Finally, in Chapter \ref{ch:conclusions}, we discuss and summarize our findings, also providing a discussion of their impacts and possible future venues of research to pursue.