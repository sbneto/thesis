\chapter{Introduction}

% Here I intent to say that we miss behavior and make mistakes when dealing with user data
% Grab supporting research from both intros and maybe look for more
% Add overview: Chapter for what we miss (escience2015) and Chapter for our mistakes (www2016)
% Decide for an overall conclusion or a ``per chapter'' conclusion, with a smaller overall conclusion

\section{Better Understanding Users' Behavior}

Users' behavior span over a multitude of actions on social networks \cite{Benevenuto2009, Gilbert2009, Comarela2012}: they search for friends, write messages, post images, videos and sounds among many other possibilities. Much of this content is captured by social networks mechanisms that explicitly tag and identify characteristics of the users' interaction. These mechanisms, however, are limited in capturing users' intention and diverse behavior, for instance, researchers have found that users might use references to content that only a specific audience from their peers can understand, turning a common context into a tool of privacy in public contexts \cite{Boyd2011}. The problem of identifying part of these interactions that are not fully captured is important to better understand users as well as to provide new perspectives on what kind of features social networks should provide.

Research has shown that not accounting for time can lead to mistakes when dealing with social networks. Just as with offline contexts, systems ans society changes over the years, as well younger generations have a different behavior from older generations. More than just considering different snapshots over time, we analyze users that joined at different stages of the network evolution. Just as with missed reactions, considering time-evolving users is important to reveal behavior that otherwise would not be noticed. Using simple cohorting methods, we demonstrate how different this behavior can be and how easy is to draw wrong conclusions from averaging practices.

In the first part of this work, we address the problem of missed reactions, proposing a text similarity method to identify missed reactions and validating it on Twitter data. In the second part of this work, we also analyze the users' evolution from a cohort perspective on the time that they joined the network. We show, in the context of Reddit, how users' behavior vary depending on the year that they joined the network as well as how misleading not accounting for time can be. 

\section{Users' Missed Reactions}

Studies on social networks often use actions people take on other people's online content as evidence of social interactions for developing their models. In domains including Usenet \cite{Joyce2006}, Wikipedia \cite{Black2011}, and Facebook \cite{Gilbert2009}, explicit replies are interpreted as evidence of interpersonal interaction and social ties.  These explicit reactions are also used in studies of influence online, such as predicting when an item is likely to be forwarded in Twitter (e.g., \cite{Suh2010,Comarela2012}).

Not all responses, however, are explicitly marked by the system.  For instance, a post that is explicitly threaded as a reply to a particular post in a discussion forum might nevertheless address another post or posts.  In Twitter, one of the study cases of this work, there are buttons for replying to and retweeting another user's tweet---but users might compose a new tweet that references another recently seen without hitting the reply button.  Users might do this for a variety of reasons, from being inspired to write their own post on a topic they see coming up in their feed to using the system in ways not intended by the designer (such as copying and pasting content into a new tweet rather than pressing a retweet button).  

Being able to identify these non-obvious, indirect responses might allow researchers to have a more accurate view of social interaction than explicit mechanisms provide.  This might also improve overall estimates of users' responsiveness to others, for instance,
at the individual level, they might indicate how desirable a user is as a follower: people might wish to have followers who are more likely to redistribute their content.  Aggregating responsiveness of a user's followers at the ego network level could support better estimates of an individual's potential reach or influence \cite{Domingos2001} based on the responsiveness of their followers.  Better responsiveness measures could also improve transmission probabilities in epidemiology-inspired models of diffusion in social networks \cite{Bakshy2012a}. 

In the first part of this work, we dedicate a chapter to assesses the prevalence of non-explicit responses in a dataset drawn from Twitter, using a measure of normalized textual similarity between a user's tweets and recent friends' tweets based on $tf\mhyphen idf$ scores.  Comparing this to the explicit responses provided by the system shows that explicit indicators of response (replies and retweets) in Twitter are in fact associated with high normalized similarity scores.  Choosing conservative score cutoffs for predicting that a tweet is a response and manually inspecting high-scoring tweets that are not marked as responses suggests that explicit indicators miss at least \highNonTaggedTweetCountPct{}\% of reactions. 
Further, this varies between users: some users systematically fail to use formal response mechanisms, meaning that these users are under-represented in studies that rely on explicit indicators of response and under-counted when considering their potential as information spreaders. These results show that the problem of non-explicit responses is an important one with practical implications for understanding interaction and influence online.

\section{Users' Hidden Behavior}

Understanding the evolution of users in a social network is essential for a variety of tasks: monitoring community health, predicting individual user trajectories, and supporting effective recommendations, among others.  Many works aim at explaining these temporal aspects of evolution. Some adopt a point of view of the whole network and try to understand general patterns of behavior \cite{Zhu2014, Kooti2010}, while others adopt a user-centric point of view and try to model \cite{Correa2010, Priedhorsky2007, Panciera2009, Welser2011} or predict \cite{Danescu-niculescu-mizil2013} individuals' behavior.

These approaches often combine all available data into aggregate analyses of the whole community over its entire history.  This can be a natural response to limitations in the amount of available data:  datasets may capture a small part of the community's history \cite{Artzi2012}; timestamps may not be available \cite{Priedhorsky2007, Pujol2010}; snapshots may provide limited views of the community \cite{Cosley2010}; or the community itself may be small \cite{Lewis2008}.  Aggregate time-based analyses are also a natural first way to address questions of community evolution.

However, we argue that many of these aggregated views are misleading. The conditions under which users join the community may vary greatly over time in ways that might impact their behavior \cite{Miller2015}.  Among other things, popularity, purpose, features, interface, and algorithms can change: Wikipedia circa 2005 and circa 2015 are very different, as are Facebook of 2005 and 2015.  Analyses---including some of our own past work---that fail to account for this change may miss important details of what is really going on.

We support this argument through an analysis of user effort in Reddit, one of the most popular and long-running online communities, based on a very large, recently released dataset of posting behavior.  We address a number of questions commonly raised about users' effort in online communities: how active are users, how hard do they work, and what kinds of things do they do?  In each case, we compare aggregate analyses of posting behavior to ones that treat users in Reddit as yearly cohorts, and views that focus on calendar time versus user-referential views that normalize behavior based on the date of a user's first visible activity.  We also look at differences within yearly cohorts, focusing on how behavior differs between shorter and longer-lived users within each cohort.

We find that these accountings for time reveal insights about Reddit beyond what commonly performed aggregate analyses can provide.  Users who join Reddit earlier post more and longer comments than those who join later, while users who survive longer start out both more active and more likely to comment than submit versus users who leave Reddit early; none of these findings are obvious from aggregate views of user behavior.  

Further, we find that aggregate analysis can be downright misleading.  For instance, although average comment length decreases over time in an aggregate view, the comment length for surviving users increases over time in every cohort.  Likewise, an aggregate analysis suggests that longer-lived users post more over time; this is not the case.  Instead, users come into Reddit as active as they will ever be (akin to Panciera et al.'s finding that Wikipedians are ``born, not made'' \cite{Panciera2009}), and the rise in average activity for surviving users over time is driven by lower-activity users leaving early.

We see the second part of this work as both making specific contributions to understanding behavior in Reddit and a more general contribution around the importance of considering change over time in analyzing online communities. 

\section{Organization of this Work}

We dedicate Chapter \ref{ch:reactions} of this work to the problem of detecting missed reaction. We propose a simple method based on text similarity to detect reactions other than retweets and replies in Twitter, and we show that a significant amount of reactions are being missed. Not only that, we also show that many users consistently react in ways that are not captured by these mechanisms.

In Chapter \ref{ch:cohorts}, we cohort users in Reddit based on their creation and survival years and we show how naive aggregation of data can lead to wrong conclusions. More than that, these simple methods can reveal evolution trends that would be otherwise missed, and they highlight the significant role of users joining and leaving the network in shaping the overall behavior.

Finally, in Chapter \ref{ch:conclusions}, we discuss and summarize our findings and provide a discussion of their impacts and possible future venues of research to pursue.