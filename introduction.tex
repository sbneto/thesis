\chapter{Introduction}

\section{A Brief History of this Thesis}

Back in 2011 I was fresh out of college and, having majored in Applied Maths and Computer Engineering, I felt ready to face a bigger challenge. I applied for my Ph.D. at the University of São Paulo after talking to Professor Roberto. By the time, we discussed a few possibilities for a thesis subject and I made clear that I wanted something industry-related. Professor Roberto then suggested that I could work with Dr. Claudio Pinhanez, a former professor at the Maths Department of the University of São Paulo, that by then was a researcher at IBM Research Brazil. I was more than pleased with this plan.

After about one year into the program, I started interning at IBM Research Brazil. It was then that this thesis started to take form. By the time, Facebook was already the major online social network and the idea that modeling users' online behavior would be essential in the near future for the majority of businesses was something I had clear in my mind. Certainly, I was not the only one to think so, and there were many researchers already studying social networks such as Twitter, Wikipedia, LastFM, among others. Recommender systems were popular, as well as studies trying to predict virality of tweets. 

My initial assignments as an intern were to craw data from the Twitter network that used for a study on the Obama's and Romney's electoral campaigns. Later on, this same data was used for the reaction detection work in this thesis. I also studied hidden Markov models in order to model users' behavior, thinking of the messages they authored as the observables for the model and their hidden states as their state of mind. Although this approach did not lead to any significant findings, it evolved into the idea of predicting weather or not a tweet message would be retweeted.

However, this idea of predicting retweets had been recently explored by the time. One thing though that was common in its related work by then was that none of them had a user-centered approach. They all gathered a lot of data and tried to find one classifier that would predict retweets or replies for any given message. As I was recently studying hidden Markov models, graphical models were fresh on my mind, and this lead to the proposal I made in my qualifying exam: create a graphical model for each user that could be used to classify his or her tweets and predict weather or not it would be retweeted/replied. 

Soon after my qualifying exam, my internship at IBM ended and I started working at a startup named GetNinjas. At the same time, I was in contact with Professor Dan Cosley from Cornell University, to whom Dr. David Millen introduced me during my time at IBM. Professor Dan agreed to have me as a foreign student for one year, and six months after the end of my internship at IBM I was on my way to Cornell. During this six months, I focused on developing a geo-constrained distribution system for GetNinjas, that was an online services marketplace.

Arriving at Cornell, I started working at the Department of Information Science, together with Professor Dan Cosley. In retrospect, now I find that my thesis would be better contextualized in the Information Sciences department rather than the Computer Science, but I only realized this later on. 

In the first months there, I worked on the research proposal I made during my qualifying exam. Unfortunately, the results of my proposed method were less than exciting: I often did not have enough data to properly train a bayesian network and when I had, sometimes my variable of interest, the retweet action, would be eliminated by a significance threshold. Discussing these issues with Dan, I dropped this approach. Soon after this I started working on a new concept, one that we had never seen anyone addressing before: sometimes users are reacting to things they have seen or been exposed to, but these reactions are not necessarily captured by the social networks explicit tools for reactions. We called these reactions implicit reactions.

Based on this idea, we proposed a method based on text similarity that would use ego-networks, a user-centered approach, to detect these implicit reactions that were missing when we addressed only retweets and replies. Since we were taking a user-centered approach, we were also able to measure how some users could be invisible if we only considered explicit reactions, and we saw that some users consistently avoided explicit reaction mechanisms, putting a lot more effort in copying and pasting rather than just clicking a "reply" or "retweet" button. These ideas were published in the 2015 e-Science conference and are part of this thesis.

Around the time I was working on the Twitter data, I learned of a large reddit dataset through another student at Cornell. Dan suggested that I explored the idea of novelty of content to predict the popularity of a new message. To do this, I time-ordered the messages in this reddit dataset and created time windows preceding these messages, trying to detect weather or not messages that had no similars in the network during this time window would be somehow more likely to propagate than messages that were more similar to content created during the time window. Unfortunately, we could not establish any correlation with novelty and the likelihood of propagating.

Moving forward, I came back to the observation I made during the Twitter work that users were varying the level effort they put in the network, and I thought that users could be optimizing their effort to reach some objective. I started trying to model users as a entities that have a bias towards some topics, and applied Latent Dirichlet Allocation to find "topics" over users. The idea was that a user could be seen as a document, and each time a user posted in a subreddit (similar to a forum in reddit, which is a collection of multiple subreddits) I could consider that this subreddit was "word" in the document. Based on this, I was able find topics of subreddits, clustering subreddits based on users' behavior. I wanted to understand how users would optimize their posting effort across the subreddits in these clusters.

While working with this idea, however, I faced many pitfalls while aggregating reddit's data (more than a billion comments), and realized that the behavior of users changed significantly over time. These pitfalls often hid important trends of growth or decrease in users' activity over time for the effort metrics I was trying to capture. We realized that these pitfalls were in fact occurrences of the Simpson's Paradox. We organized these observations and the interesting trends we discovered about how users evolved in reddit and published it in the 2016 WWW Conference. My favorite take away of this work is that the newer generations that join the social network are likely to become the majority of the network, and when looking at the average behavior of users, they will dominate the overall trend. This means that whenever you design products or strategies, you should always think more of who is going to join your network rather than who will stay in it, at least during the time it is growing exponentially.

By the time of the WWW conference, I was already back in Brazil. As soon as I arrived, GetNinjas got in touch with me and offered me a position as a Data Scientist, which I accepted. I began by working with their marketing system, which was primarily focused on AdWords by the time. This gave me a very clear idea of how to put all the things I had learned in practice: modeling users is a extremely powerful tool to optimize marketing strategies. I developed a bid optimization system and a campaign creation system for geo-constrained business. By the time of the defense of this thesis, the FAPESP research funding agency accepted our proposal to fund the development of this system through the PIPE program\footnote{This is a program to help develop research and new technology in small and medium business. More information can be found in \url{http://www.fapesp.br/pipe/}.}.

This summarizes the trajectory of how this thesis came to be and how the ideas presented here are connected in a larger context. I am greatly thankful for all the people that helped me and made all this possible.

\section{Better Understanding Users' Behavior}

Users' behavior span over a multitude of actions on social networks \cite{Benevenuto2009, Gilbert2009, Comarela2012}: they search for friends, write messages, post images, videos and sounds among many other possibilities. Much of this content is captured by social networks mechanisms that explicitly tag and identify characteristics of the users' interaction. These mechanisms, however, are limited in capturing users' intention and diverse behavior. For instance, researchers have found that users might use references to content that only a specific audience from their peers can understand, turning a common context into a tool of privacy in public contexts \cite{Boyd2011}. The problem of identifying part of these interactions that are not fully captured is important to better understand users as well as to provide new perspectives on what kind of features social networks should provide. This is the main issue addressed in this thesis.

Research has shown that not accounting for time can lead to mistakes when dealing with social networks. Just as with offline contexts, systems and society changes over the years, as well younger generations have a different behavior from older generations. More than just considering different snapshots over time, we analyze users that joined at different stages of the network evolution. Just as with missed reactions, considering time-evolving users is important to reveal behavior that otherwise would not be noticed. Using simple cohorting methods, we demonstrate how different this behavior can be and how easy it is to draw wrong conclusions from averaging practices.

In the first part of this thesis, we address the problem of missed reactions, proposing a text similarity method to identify missed reactions and validating it on Twitter data. We show that a considerable number of users' reactions are not captured by current mechanisms in Twitter and that some types of users are significantly underrepresented based solely on these metrics. In the second part of this thesis, we analyze the users' evolution from a cohort perspective built on top of the time that they joined the network. We show, in the context of Reddit, how users' behavior vary depending on the year that they joined the network as well as how misleading not accounting for time can be. 

This thesis was made in collaboration with IBM Research Brazil and Cornell University. The author interned at IBM Research for 2 years under the advisement of Claudio Pinhanez and visited Cornell University for 1 year as part of the sandwich Ph.D. program, under the advisement of Dan Cosley.

\section{Objectives}

The objective of this thesis is to improve our understanding of users' behavior on social networks. More specifically, to understand the missed behavior in the form of reactions that are not captured and temporal trends that are missed due to poor aggregation of data. We developed methods that help us to capture and identify missed reactions and propose analysis tools that allow us to avoid misunderstanding our data.

\section{Contributions}

We can summarize the contributions of this thesis as the following:

\begin{itemize}
	\item Proposal of a new problem: understanding users' indirect reactions.
	\item Development of a method to detect indirect reactions in the context of Twitter, revealing about 11\% of missed reactions, as well as patterns of behavior that are common, but not modeled, e.g., group conversations.
	\item A cohorted view of the users' evolution on Reddit over 7 years, revealing different behavioral patterns as a function of the users' tenure in the network.
	\item Practical examples of common aggregation practices that lead to wrong conclusions when dealing with time series.
\end{itemize}

The ideas proposed in this thesis have been published in \cite{BarbosaNeto2013, Barbosa, Barbosa2016}.

\section{Organization of this Thesis}

We dedicate Chapter \ref{ch:reactions} of this thesis to the problem of detecting missed reaction. We propose a method based on text similarity to detect reactions other than retweets and replies in Twitter. We show that a significant amount of reactions are being missed. Furthermore, we show that many users consistently react in ways that are not captured by these mechanisms.

In Chapter \ref{ch:cohorts}, we cohort users in Reddit based on their creation and survival years. This analysis shows that naive aggregation of data can lead to wrong conclusions. It is shown that the proposed method can reveal users' evolution trends that would be otherwise missed. The proposed approach highlights the significant role of users joining and leaving the network in shaping the overall behavior.

Finally, in Chapter \ref{ch:conclusions}, we discuss and summarize our findings, also providing a discussion of their impacts and possible future venues of research to pursue.