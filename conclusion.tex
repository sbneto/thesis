\chapter{Conclusions and Final Remarks}
\label{ch:conclusions}

\section{Conclusion and Future Work escience15}

This paper presented a novel method of capturing some of a user's non-explicit reactions to followees' content in Twitter by using text similarity scores between a user's tweets and those of their followees.  The analysis indicates that the method does generate higher scores on average for system tagged Replies and Retweets than Non-Tagged tweets, suggesting that it captures real signal about responses (\ref{rq:similarityPotential}).  Using a conservative cutoff for predicting whether a non-tagged tweet is a response suggests that at least \highNonTaggedTweetCountPct{}\% of actual responses are not tagged by the system.  These responses are distributed across almost a quarter of the users in the dataset, with a quarter of those having more missed reaction messages than explicit system tagged ones. These are not just naive, low-activity users who do not understand Twitter and might be ignored in analysis; a number of these users are quite active, with dozens or hundreds of tweets in a 14-day window (\ref{rq:usersDistribution}).  

Although the method has provided useful insights into the prevalence of non-explicit replies in Twitter, it is a coarse model.  It tends to under-evaluate Replies; is more sensitive to Retweet size than desirable; likely misses a number of non-explicit responses that have lower scores but are nonetheless real responses to the feed; and doesn't address responses to content outside the feed such as views by hashtag or username.  Ongoing work aims at addressing these limitations by improving the quality of the scoring function.  One natural way of improving the scoring function is to incorporate other relevant social features highlighted by past work (Table~\ref{tab:characteristics}).  We expect that better models of language, network characteristics, and attention that build on these features would give better estimates of how people react to content produced by their followees.

Another possible unfolding research topic is how to use these reaction scores to understand the reaction patterns and estimate the individual reaction level for each user.  This is important for effective models of diffusion at all levels, from understanding when adding an individual to a follower network might be most valuable, to estimating the overall reach of an individual's network, to modeling diffusion of information in the large.  Missing \highNonTaggedTweetCountPct{}\% of responses and \usersAboveLinePct{}\% users is a substantial amount of error to bear for such models, making the identification of non-explicit responses an important problem to pursue.


\section{Discussion and Future Work www16}

\subsection{Limitations and Future Work}

In this paper we focused our attention on visible behavior attributable to specific users, which in this dataset meant submissions and comments.  As with many analyses that focus on visible behavior, this means we miss important phenomena.  In particular, we discount lurkers despite their known importance as audience members \cite{Nonnecke2003} and potential future contributors \cite{Ridings2006}.  Many lurkers likely vote, and thus lurking may be even more important in a context like Reddit where votes affect content visibility and provide explicit markers of attention and reputation.  

However, the dataset does not have information on individual voters or timestamps, just the aggregate number of votes a post had received at the time of the crawl, making it impossible to use them as activity measures for specific users.  The existing voting data might be much more useful, however, in addressing questions that involve predicting a given user's future behavior based on how other users respond to a user's early contributions \cite{Joyce2006,Sarkar2012}.

Focusing on visible activity can lead to blind spots in other places, as well.  In particular, our emphasis on active users led us to ignore questions of survival, leaving, and rejoining.  This was a reasonable view of the community based on the questions we were asking, but our results should all be interpreted in the context of ``given the set of active users at any given time''.  Applying these results to questions that require considering all users would be a mistake.  

We did, implicitly, consider survival in the analyses that broke cohort down by survival time; more generally, we see careful thinking about what it means to ``survive'' in a community as an interesting problem in its own right.  Many analyses assume that a gap of some time period implies that a user has left, or that users ``die'' on their last visible day of activity.  However, long gaps are common in real behavior.  People temporarily quit social media all the time \cite{Baumer2013}, and in Wikipedia, the practice of leaving temporarily is so common it has a name: ``wikibreak''.    Rather than an annoying right censorship statistical problem, this question of what it means when contributors to a community start and stop might pose a much more central issue, as a community's survival might not depend only in its ability to attract and retain users, but also in the ability to ``resurrect'' old users and leverage ``bursty'' ones.

Further, One of our assumptions was that one account is associated with one user. This might not be the case, as more than one user can share the same account \cite{Lampinen2014} or one user can have multiple accounts \cite{Bergstrom2011}.  Multiple accounts can have many functions, including making points someone doesn't want connected with their main identity, trolling or harming other users or the community, or simulating users who agree with a main identity (``sock puppets'').  While we think this is not the main driver of our results, this should be checked in future work---and sockpuppet detection and account deanonymization is an interesting question in its own right.

Finally, focusing on visible activity can also lead to blind spots around deleted content or communities.  At least in Reddit, activity from users is marked with a username of ``[deleted]'', which we discovered after realizing that one author had millions of comments(!), and that allowed us to consciously choose to exclude that data.  However, in some contexts, such as Wikipedia articles that are deleted, that activity is invisible as edit behavior on those articles does not show up in many data dumps.  Such invisible activity might be important in understanding either individual users or the community. 

\section{Conclusions}

This work highlights the importance of taking time into consideration when analyzing users' evolution in social networks. We do so by cohorting the users based on their creation year. Although simple, this approach provides a number of insights that would be missed by straightforward aggregate analysis methods.  We also analyze the evolution of users and communities from a shifted time referential: considering the time of an action in relation to the user creation date. This also reveals unexpected phenomena that we would otherwise not notice.

While analyzing how the amount of posting changes over time (\ref{rq:activityChange}), we found that user posting activity for surviving Reddit users is actually significantly higher than a naive average would suggest, that older users who survive are considerably more active than younger survivors, and that these newer users are unlikely to catch up (\ref{srq:newFollowOld}).  Controlling for survival provided evidence for hypothesis (\ref{hyp:lowActivityLeave}), that users have a stable level of posting activity over time (with slightly decreasing patterns).  Further, the percentage of surviving but low-activity users is increasing in the younger cohorts 

When looking at changes in comment length over time (\ref{rq:commentLengthOverTime}) as a proxy for users' effort, we found that while the overall average in Reddit seems to decrease, users actually write longer comments as they survive, no matter when they join.  However, later cohorts of users that joined the network are writing smaller comments; their greater number leads to an instance of Simpson's paradox, where the overall average decreases while the series for each individual cohort increases. 

Finally, we analyze whether users change their commenting versus submission behavior over time (\ref{rq:typeOfActivity}). 
We found that users with a higher initial comment to submission ratio survive longer on average, and that this ratio increases for surviving users, particularly for earlier cohorts.  This isn't because activity rises overall, as posting activity remains stable; instead, it suggests that longer-term users substitute commenting for submissions. 

An important remark of this paper is how different demographics of users joining and leaving a network play a significant role in shaping the average user behavior. Failing to account for these might limit our interpretation of the data (\ref{hyp:survivingMorePosts}, \ref{hyp:decreasingCommitment} or \ref{hyp:communityNorm}) and lead to wrong conclusions.

Both our and work and its limitations suggest fruitful directions for better understanding of users' evolution in both Reddit and online communities in general, directions we hope inspire other work in this area.  
